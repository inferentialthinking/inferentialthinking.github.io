{"version":2,"kind":"Notebook","sha256":"7dc7cb7596020241c548d773e22a1998477507871bec64f34da79d9126b9f289","slug":"chapters.17.5.accuracy-of-the-classifier","location":"/chapters/17/5/Accuracy_of_the_Classifier.ipynb","dependencies":[],"frontmatter":{"title":"The Accuracy of the Classifier","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"license":{"content":{"id":"CC-BY-NC-ND-4.0","url":"https://creativecommons.org/licenses/by-nc-nd/4.0/","name":"Creative Commons Attribution Non Commercial No Derivatives 4.0 International","CC":true}},"github":"https://github.com/data-8/textbook","numbering":{"title":{"enabled":true,"offset":1}},"source_url":"https://github.com/data-8/textbook/blob/main/chapters/17/5/Accuracy_of_the_Classifier.ipynb","edit_url":"https://github.com/data-8/textbook/edit/main/chapters/17/5/Accuracy_of_the_Classifier.ipynb","enumerator":"17.5","thumbnail":"/build/5e548385d4da863b59d51148bd5d0eeb.jpeg","exports":[{"format":"ipynb","filename":"Accuracy_of_the_Classifier.ipynb","url":"/build/Accuracy_of_the_Clas-d785cc080424065eb90b13bb873796c9.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"To see how well our classifier does, we might put 50% of the data into the training set and the other 50% into the test set.  Basically, we are setting aside some data for later use, so we can use it to measure the accuracy of our classifier.  We’ve been calling that the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"VP2yp8HK8u"},{"type":"emphasis","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"test set","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"DEXniLuxZr"}],"key":"LK9u8KsyQb"},{"type":"text","value":". Sometimes people will call the data that you set aside for testing a ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"BroHMQuz4z"},{"type":"emphasis","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"hold-out set","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"SSWfqQlher"}],"key":"XMGizZbDwP"},{"type":"text","value":", and they’ll call this strategy for estimating accuracy the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"cvWGeJsQhd"},{"type":"emphasis","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"hold-out method","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"tdh4JR0Z2P"}],"key":"v26ccSUWYe"},{"type":"text","value":".","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"DEjM877I7U"}],"key":"aCsLLROJuY"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Note that this approach requires great discipline.  Before you start applying machine learning methods, you have to take some of your data and set it aside for testing.  You must avoid using the test set for developing your classifier: you shouldn’t use it to help train your classifier or tweak its settings or for brainstorming ways to improve your classifier.  Instead, you should use it only once, at the very end, after you’ve finalized your classifier, when you want an unbiased estimate of its accuracy.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"RG12tT9bzL"}],"key":"TdBNgTaV42"}],"key":"hSBMEGuKcS"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"import matplotlib\n#matplotlib.use('Agg')\npath_data = '../../../assets/data/'\nfrom datascience import *\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport math\nimport scipy.stats as stats\nplt.style.use('fivethirtyeight')","visibility":"remove","key":"qgK3AwTOzk"},{"type":"output","id":"CSfe1nbZKRNnWaj5BEoSK","data":[],"visibility":"show","key":"sAmajlYc81"}],"visibility":"show","key":"AgUO06bSqZ"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"def distance(point1, point2):\n    \"\"\"Returns the distance between point1 and point2\n    where each argument is an array \n    consisting of the coordinates of the point\"\"\"\n    return np.sqrt(np.sum((point1 - point2)**2))\n\ndef all_distances(training, new_point):\n    \"\"\"Returns an array of distances\n    between each point in the training set\n    and the new point (which is a row of attributes)\"\"\"\n    attributes = training.drop('Class')\n    def distance_from_point(row):\n        return distance(np.array(new_point), np.array(row))\n    return attributes.apply(distance_from_point)\n\ndef table_with_distances(training, new_point):\n    \"\"\"Augments the training table \n    with a column of distances from new_point\"\"\"\n    return training.with_column('Distance', all_distances(training, new_point))\n\ndef closest(training, new_point, k):\n    \"\"\"Returns a table of the k rows of the augmented table\n    corresponding to the k smallest distances\"\"\"\n    with_dists = table_with_distances(training, new_point)\n    sorted_by_distance = with_dists.sort('Distance')\n    topk = sorted_by_distance.take(np.arange(k))\n    return topk\n\ndef majority(topkclasses):\n    ones = topkclasses.where('Class', are.equal_to(1)).num_rows\n    zeros = topkclasses.where('Class', are.equal_to(0)).num_rows\n    if ones > zeros:\n        return 1\n    else:\n        return 0\n\ndef classify(training, new_point, k):\n    closestk = closest(training, new_point, k)\n    topkclasses = closestk.select('Class')\n    return majority(topkclasses)","visibility":"remove","key":"bTM1RxSmGK"},{"type":"output","id":"vAgxkKq8OcNjBleUA4seg","data":[],"visibility":"show","key":"cZNeRd3NIy"}],"visibility":"show","key":"t8TSEVSxMe"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"wine = Table.read_table(path_data + 'wine.csv')\n\n# For converting Class to binary\n\ndef is_one(x):\n    if x == 1:\n        return 1\n    else:\n        return 0\n    \nwine = wine.with_column('Class', wine.apply(is_one, 0))","visibility":"remove","key":"QVxDjIFn5U"},{"type":"output","id":"uZ7hqLkh7_NGTm3sMPZBA","data":[],"visibility":"show","key":"tEbxn26ZOk"}],"visibility":"show","key":"ikvQ6NuJ0C"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Measuring the Accuracy of Our Wine Classifier","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fjhaDXxOQS"}],"identifier":"measuring-the-accuracy-of-our-wine-classifier","label":"Measuring the Accuracy of Our Wine Classifier","html_id":"measuring-the-accuracy-of-our-wine-classifier","implicit":true,"key":"s3TpxVAkSL"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"OK, so let’s apply the hold-out method to evaluate the effectiveness of the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"W7UX3IAEiN"},{"type":"inlineMath","value":"k","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span></span></span></span>","key":"DstxB4u4LK"},{"type":"text","value":"-nearest neighbor classifier for identifying wines.  The data set has 178 wines, so we’ll randomly permute the data set and put 89 of them in the training set and the remaining 89 in the test set.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"tARWEf0Zuh"}],"key":"fmoxaowhb3"}],"key":"PvROyXinqa"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"shuffled_wine = wine.sample(with_replacement=False) \ntraining_set = shuffled_wine.take(np.arange(89))\ntest_set  = shuffled_wine.take(np.arange(89, 178))","key":"H3j4EpsHye"},{"type":"output","id":"SCh7WiiHaG8DAvX_FBqS5","data":[],"key":"rglXzesc0w"}],"key":"jpMTY7itTD"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We’ll train the classifier using the 89 wines in the training set, and evaluate how well it performs on the test set. To make our lives easier, we’ll write a function to evaluate a classifier on every wine in the test set:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sjXj2jMRtm"}],"key":"zqPrGIodph"}],"key":"sT2p5haeV0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def count_zero(array):\n    \"\"\"Counts the number of 0's in an array\"\"\"\n    return len(array) - np.count_nonzero(array)\n\ndef count_equal(array1, array2):\n    \"\"\"Takes two numerical arrays of equal length\n    and counts the indices where the two are equal\"\"\"\n    return count_zero(array1 - array2)\n\ndef evaluate_accuracy(training, test, k):\n    test_attributes = test.drop('Class')\n    def classify_testrow(row):\n        return classify(training, row, k)\n    c = test_attributes.apply(classify_testrow)\n    return count_equal(c, test.column('Class')) / test.num_rows","key":"I1pxznqRa7"},{"type":"output","id":"TLKtStYRWzvTGn2oOeaNt","data":[],"key":"DF2dNPb4VO"}],"key":"QWqU5LmV7b"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now for the grand reveal -- let’s see how we did.  We’ll arbitrarily use ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xsMeOpr6pR"},{"type":"inlineMath","value":"k=5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">k=5</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">5</span></span></span></span>","key":"AHBgXDsvHp"},{"type":"text","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kmry25oXmD"}],"key":"kAJPAl6VmD"}],"key":"lUprP4pn7s"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"evaluate_accuracy(training_set, test_set, 5)","key":"Q8022sW5dt"},{"type":"output","id":"sQrg6zxtHtW0Gzs4dUEZ2","data":[{"output_type":"execute_result","execution_count":6,"metadata":{},"data":{"text/plain":{"content":"0.898876404494382","content_type":"text/plain"}}}],"key":"oTO7pN8IF7"}],"key":"Xn4e7IVOPe"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The accuracy rate isn’t bad at all for a simple classifier.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uGLrzBJqjH"}],"key":"p4kDAju0LJ"}],"key":"hImQCNhNXA"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Breast Cancer Diagnosis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WKCOGjn1nm"}],"identifier":"breast-cancer-diagnosis","label":"Breast Cancer Diagnosis","html_id":"breast-cancer-diagnosis","implicit":true,"key":"W7eSCM9dWp"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Now I want to do an example based on diagnosing breast cancer.  I was inspired by Brittany Wenger, who won the Google national science fair in 2012 as a 17-year old high school student.  Here’s Brittany:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"monkWWm1au"}],"key":"L4eYwNaVyy"},{"type":"image","url":"/build/5e548385d4da863b59d51148bd5d0eeb.jpeg","alt":"Brittany Wenger","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"W0of7E1Dum","urlSource":"http://i.huffpost.com/gen/701499/thumbs/o-GSF83-570.jpg?3"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Brittany’s ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"F6YRj0WuSS"},{"type":"link","url":"https://sites.google.com/a/googlesciencefair.com/science-fair-2012-project-64a91af142a459cfb486ed5cb05f803b2eb41354-1333130785-87/home","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"science fair project","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"x5rvyG1IBi"}],"urlSource":"https://sites.google.com/a/googlesciencefair.com/science-fair-2012-project-64a91af142a459cfb486ed5cb05f803b2eb41354-1333130785-87/home","key":"ztTUSurvS7"},{"type":"text","value":" was to build a classification algorithm to diagnose breast cancer.  She won grand prize for building an algorithm whose accuracy was almost 99%.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"kImSnAqZbj"}],"key":"AtaU4R6CWF"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Let’s see how well we can do, with the ideas we’ve learned in this course.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"xzk09QG2by"}],"key":"CPGqYPpbFI"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"So, let me tell you a little bit about the data set.  Basically, if a woman has a lump in her breast, the doctors may want to take a biopsy to see if it is cancerous.  There are several different procedures for doing that.  Brittany focused on fine needle aspiration (FNA), because it is less invasive than the alternatives.  The doctor gets a sample of the mass, puts it under a microscope, takes a picture, and a trained lab tech analyzes the picture to determine whether it is cancer or not.  We get a picture like one of the following:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"B34LajHwZG"}],"key":"FNodvmuQj2"},{"type":"image","url":"/build/benign-9219f7c2027a6b2e9cdb5c1f8e371268.png","alt":"benign","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"q4Wzm1L8d8","urlSource":"../../../images/benign.png"},{"type":"image","url":"/build/malignant-906ec53ee2ad5b9b6d21b52f801ab7fc.png","alt":"cancer","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"mTYpz4SeJC","urlSource":"../../../images/malignant.png"},{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Unfortunately, distinguishing between benign vs malignant can be tricky.  So, researchers have studied the use of machine learning to help with this task.  The idea is that we’ll ask the lab tech to analyze the image and compute various attributes: things like the typical size of a cell, how much variation there is among the cell sizes, and so on.  Then, we’ll try to use this information to predict (classify) whether the sample is malignant or not.  We have a training set of past samples from women where the correct diagnosis is known, and we’ll hope that our machine learning algorithm can use those to learn how to predict the diagnosis for future samples.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"v4Odzcl8nJ"}],"key":"ESJMZin8DI"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"We end up with the following data set.  For the “Class” column, 1 means malignant (cancer); 0 means benign (not cancer).","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"T9VtOKtiOu"}],"key":"xwOHQEaUaK"}],"key":"vJnywGNf5e"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"patients = Table.read_table(path_data + 'breast-cancer.csv').drop('ID')\npatients","key":"iIuUgKrw2f"},{"type":"output","id":"xbG3p6dmFCjUhp5sSbKJP","data":[{"output_type":"execute_result","execution_count":7,"metadata":{},"data":{"text/html":{"content":"<table border=\"1\" class=\"dataframe\">\n    <thead>\n        <tr>\n            <th>Clump Thickness</th> <th>Uniformity of Cell Size</th> <th>Uniformity of Cell Shape</th> <th>Marginal Adhesion</th> <th>Single Epithelial Cell Size</th> <th>Bare Nuclei</th> <th>Bland Chromatin</th> <th>Normal Nucleoli</th> <th>Mitoses</th> <th>Class</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>5              </td> <td>1                      </td> <td>1                       </td> <td>1                </td> <td>2                          </td> <td>1          </td> <td>3              </td> <td>1              </td> <td>1      </td> <td>0    </td>\n        </tr>\n        <tr>\n            <td>5              </td> <td>4                      </td> <td>4                       </td> <td>5                </td> <td>7                          </td> <td>10         </td> <td>3              </td> <td>2              </td> <td>1      </td> <td>0    </td>\n        </tr>\n        <tr>\n            <td>3              </td> <td>1                      </td> <td>1                       </td> <td>1                </td> <td>2                          </td> <td>2          </td> <td>3              </td> <td>1              </td> <td>1      </td> <td>0    </td>\n        </tr>\n        <tr>\n            <td>6              </td> <td>8                      </td> <td>8                       </td> <td>1                </td> <td>3                          </td> <td>4          </td> <td>3              </td> <td>7              </td> <td>1      </td> <td>0    </td>\n        </tr>\n        <tr>\n            <td>4              </td> <td>1                      </td> <td>1                       </td> <td>3                </td> <td>2                          </td> <td>1          </td> <td>3              </td> <td>1              </td> <td>1      </td> <td>0    </td>\n        </tr>\n        <tr>\n            <td>8              </td> <td>10                     </td> <td>10                      </td> <td>8                </td> <td>7                          </td> <td>10         </td> <td>9              </td> <td>7              </td> <td>1      </td> <td>1    </td>\n        </tr>\n        <tr>\n            <td>1              </td> <td>1                      </td> <td>1                       </td> <td>1                </td> <td>2                          </td> <td>10         </td> <td>3              </td> <td>1              </td> <td>1      </td> <td>0    </td>\n        </tr>\n        <tr>\n            <td>2              </td> <td>1                      </td> <td>2                       </td> <td>1                </td> <td>2                          </td> <td>1          </td> <td>3              </td> <td>1              </td> <td>1      </td> <td>0    </td>\n        </tr>\n        <tr>\n            <td>2              </td> <td>1                      </td> <td>1                       </td> <td>1                </td> <td>2                          </td> <td>1          </td> <td>1              </td> <td>1              </td> <td>5      </td> <td>0    </td>\n        </tr>\n        <tr>\n            <td>4              </td> <td>2                      </td> <td>1                       </td> <td>1                </td> <td>2                          </td> <td>1          </td> <td>2              </td> <td>1              </td> <td>1      </td> <td>0    </td>\n        </tr>\n    </tbody>\n</table>\n<p>... (673 rows omitted)</p>","content_type":"text/html"},"text/plain":{"content":"Clump Thickness | Uniformity of Cell Size | Uniformity of Cell Shape | Marginal Adhesion | Single Epithelial Cell Size | Bare Nuclei | Bland Chromatin | Normal Nucleoli | Mitoses | Class\n5               | 1                       | 1                        | 1                 | 2                           | 1           | 3               | 1               | 1       | 0\n5               | 4                       | 4                        | 5                 | 7                           | 10          | 3               | 2               | 1       | 0\n3               | 1                       | 1                        | 1                 | 2                           | 2           | 3               | 1               | 1       | 0\n6               | 8                       | 8                        | 1                 | 3                           | 4           | 3               | 7               | 1       | 0\n4               | 1                       | 1                        | 3                 | 2                           | 1           | 3               | 1               | 1       | 0\n8               | 10                      | 10                       | 8                 | 7                           | 10          | 9               | 7               | 1       | 1\n1               | 1                       | 1                        | 1                 | 2                           | 10          | 3               | 1               | 1       | 0\n2               | 1                       | 2                        | 1                 | 2                           | 1           | 3               | 1               | 1       | 0\n2               | 1                       | 1                        | 1                 | 2                           | 1           | 1               | 1               | 5       | 0\n4               | 2                       | 1                        | 1                 | 2                           | 1           | 2               | 1               | 1       | 0\n... (673 rows omitted)","content_type":"text/plain"}}}],"key":"RSxTG3BnRb"}],"key":"TyYQlSxiqA"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So we have 9 different attributes.  I don’t know how to make a 9-dimensional scatterplot of all of them, so I’m going to pick two and plot them:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rtwhFJ5nzH"}],"key":"fAzNLBrZAI"}],"key":"PJlrbEAoKd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"color_table = Table().with_columns(\n    'Class', make_array(1, 0),\n    'Color', make_array('darkblue', 'gold')\n)\npatients_with_colors = patients.join('Class', color_table)","key":"EHnf3aww6D"},{"type":"output","id":"Hexmc_cT1i3VgGQoTFzkf","data":[],"key":"DdAEoiRWfP"}],"key":"v8OB0qlbaK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"patients_with_colors.scatter('Bland Chromatin', 'Single Epithelial Cell Size', group='Color')","key":"BLvTAsgZDv"},{"type":"output","id":"Nzax2wZII5c3-72W3bjL3","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"04414e61459f9c4542b6559190d24a81","path":"/build/04414e61459f9c4542b6559190d24a81.png"},"text/plain":{"content":"<Figure size 360x360 with 1 Axes>","content_type":"text/plain"}}}],"key":"jXKPYihknz"}],"key":"YsiIJpw2vT"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Oops.  That plot is utterly misleading, because there are a bunch of points that have identical values for both the x- and y-coordinates.  To make it easier to see all the data points, I’m going to add a little bit of random jitter to the x- and y-values.  Here’s how that looks:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TjhHZm0vy8"}],"key":"NoTNcBtXW5"}],"key":"eCwjHW4KCB"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"def randomize_column(a):\n    return a + np.random.normal(0.0, 0.09, size=len(a))\nTable().with_columns(\n        'Bland Chromatin (jittered)', \n        randomize_column(patients.column('Bland Chromatin')),\n        'Single Epithelial Cell Size (jittered)', \n        randomize_column(patients.column('Single Epithelial Cell Size')),\n        'Class', patients.column('Class')\n    ).join('Class', color_table).scatter(1, 2, group='Color')","visibility":"remove","key":"tmPvLLxOWo"},{"type":"output","id":"Vpw6YellrxWHa_paz2T8B","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"6aec59bee719a99db8a9dab0faa0287c","path":"/build/6aec59bee719a99db8a9dab0faa0287c.png"},"text/plain":{"content":"<Figure size 360x360 with 1 Axes>","content_type":"text/plain"}}}],"visibility":"show","key":"JJ2DuejSWW"}],"visibility":"show","key":"DXI6GjL2BK"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"For instance, you can see there are lots of samples with chromatin = 2 and epithelial cell size = 2; all non-cancerous.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VIL59vmDE5"}],"key":"wiI2JuPgpX"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Keep in mind that the jittering is just for visualization purposes, to make it easier to get a feeling for the data.  We’re ready to work with the data now, and we’ll use the original (unjittered) data.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"tYcKky1OFU"}],"key":"dJE4DOEMjM"}],"key":"a3nnrBfEpk"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"First we’ll create a training set and a test set. The data set has 683 patients, so we’ll randomly permute the data set and put 342 of them in the training set and the remaining 341 in the test set.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oFXBZ3o6Dz"}],"key":"gzUNbOXDD5"}],"key":"DU7vnIPcbe"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"shuffled_patients = patients.sample(683, with_replacement=False) \ntraining_set = shuffled_patients.take(np.arange(342))\ntest_set  = shuffled_patients.take(np.arange(342, 683))","key":"P2yYHHi1I5"},{"type":"output","id":"HUum23uaAhFryM_i_exri","data":[],"key":"ubAigvpQ6P"}],"key":"BLfFkZsKKN"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s stick with 5 nearest neighbors, and see how well our classifier does.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xGfhcxJ2iU"}],"key":"wdqZnvDOdF"}],"key":"io9UOorflr"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"evaluate_accuracy(training_set, test_set, 5)","key":"vSBttKZWff"},{"type":"output","id":"1yIwoOeCZC7pILx_oa_Hh","data":[{"output_type":"execute_result","execution_count":12,"metadata":{},"data":{"text/plain":{"content":"0.967741935483871","content_type":"text/plain"}}}],"key":"Yo74ZpRIFy"}],"key":"qBfaNGPfK9"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Over 96% accuracy.  Not bad!  Once again, pretty darn good for such a simple technique.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rVTmWyuvMq"}],"key":"FTlizKTR1i"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"As a footnote, you might have noticed that Brittany Wenger did even better.  What techniques did she use? One key innovation is that she incorporated a confidence score into her results: her algorithm had a way to determine when it was not able to make a confident prediction, and for those patients, it didn’t even try to predict their diagnosis.  Her algorithm was 99% accurate on the patients where it made a prediction -- so that extension seemed to help quite a bit.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"hIUfIbhGKr"}],"key":"t3ZM4i6YT1"}],"key":"wqesz8uuS8"}],"key":"dBIlFzF5Cq"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Implementing the Classifier","url":"/chapters/17/4/implementing-the-classifier","group":"Computational and Inferential Thinking"},"next":{"title":"Multiple Regression","url":"/chapters/17/6/multiple-regression","group":"Computational and Inferential Thinking"}}},"domain":"http://localhost:3000"}