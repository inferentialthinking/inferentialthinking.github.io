
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>17.4. Implementing the Classifier &#8212; Computational and Inferential Thinking</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/book.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="canonical" href="https://inferentialthinking.org/chapters/17/4/Implementing_the_Classifier.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="17.5. The Accuracy of the Classifier" href="../5/Accuracy_of_the_Classifier.html" />
    <link rel="prev" title="17.3. Rows of Tables" href="../3/Rows_of_Tables.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://inferentialthinking.org/chapters/17/4/Implementing_the_Classifier.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Implementing the Classifier" />
<meta property="og:description" content="Implementing the Classifier  We are now ready to implement a k-nearest neighbor classifier based on multiple attributes. We have used only two attributes so far" />
<meta property="og:image"       content="https://inferentialthinking.org/_static/favicon.png" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/favicon.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Computational and Inferential Thinking</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../01/what-is-data-science.html">
   1. Data Science
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../../01/1/intro.html">
     1.1. Introduction
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../../01/1/1/computational-tools.html">
       1.1.1. Computational Tools
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../01/1/2/statistical-techniques.html">
       1.1.2. Statistical Techniques
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../01/2/why-data-science.html">
     1.2. Why Data Science?
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../../01/3/Plotting_the_Classics.html">
     1.3. Plotting the Classics
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../../01/3/1/Literary_Characters.html">
       1.3.1. Literary Characters
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../01/3/2/Another_Kind_Of_Character.html">
       1.3.2. Another Kind of Character
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../02/causality-and-experiments.html">
   2. Causality and Experiments
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../02/1/observation-and-visualization-john-snow-and-the-broad-street-pump.html">
     2.1. John Snow and the Broad Street Pump
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02/2/snow-s-grand-experiment.html">
     2.2. Snow’s “Grand Experiment”
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02/3/establishing-causality.html">
     2.3. Establishing Causality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02/4/randomization.html">
     2.4. Randomization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02/5/endnote.html">
     2.5. Endnote
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../03/programming-in-python.html">
   3. Programming in Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../03/1/Expressions.html">
     3.1. Expressions
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../../03/2/Names.html">
     3.2. Names
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../../03/2/1/Growth.html">
       3.2.1. Example: Growth Rates
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../03/3/Calls.html">
     3.3. Call Expressions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../03/4/Introduction_to_Tables.html">
     3.4. Introduction to Tables
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../04/Data_Types.html">
   4. Data Types
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../04/1/Numbers.html">
     4.1. Numbers
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../../04/2/Strings.html">
     4.2. Strings
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../../04/2/1/String_Methods.html">
       4.2.1. String Methods
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../04/3/Comparison.html">
     4.3. Comparisons
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../05/Sequences.html">
   5. Sequences
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../05/1/Arrays.html">
     5.1. Arrays
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../05/2/Ranges.html">
     5.2. Ranges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../05/3/More_on_Arrays.html">
     5.3. More on Arrays
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../06/Tables.html">
   6. Tables
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../06/1/Sorting_Rows.html">
     6.1. Sorting Rows
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../06/2/Selecting_Rows.html">
     6.2. Selecting Rows
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../06/3/Example_Trends_in_the_Population_of_the_United_States.html">
     6.3. Example: Population Trends
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../06/4/Example_Gender_Ratio_in_the_US_Population.html">
     6.4. Example: Trends in Gender
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../07/Visualization.html">
   7. Visualization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../07/1/Visualizing_Categorical_Distributions.html">
     7.1. Categorical Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../07/2/Visualizing_Numerical_Distributions.html">
     7.2. Numerical Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../07/3/Overlaid_Graphs.html">
     7.3. Overlaid Graphs
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../08/Functions_and_Tables.html">
   8. Functions and Tables
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../08/1/Applying_a_Function_to_a_Column.html">
     8.1. Applying Functions to Columns
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../08/2/Classifying_by_One_Variable.html">
     8.2. Classifying by One Variable
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../08/3/Cross-Classifying_by_More_than_One_Variable.html">
     8.3. Cross-Classifying
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../08/4/Joining_Tables_by_Columns.html">
     8.4. Joining Tables by Columns
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../08/5/Bike_Sharing_in_the_Bay_Area.html">
     8.5. Bike Sharing in the Bay Area
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../09/Randomness.html">
   9. Randomness
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../09/1/Conditional_Statements.html">
     9.1. Conditional Statements
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../09/2/Iteration.html">
     9.2. Iteration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../09/3/Simulation.html">
     9.3. Simulation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../09/4/Monty_Hall_Problem.html">
     9.4. The Monty Hall Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../09/5/Finding_Probabilities.html">
     9.5. Finding Probabilities
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../10/Sampling_and_Empirical_Distributions.html">
   10. Sampling and Empirical Distributions
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../10/1/Empirical_Distributions.html">
     10.1. Empirical Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../10/2/Sampling_from_a_Population.html">
     10.2. Sampling from a Population
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../10/3/Empirical_Distribution_of_a_Statistic.html">
     10.3. Empirical Distibution of a Statistic
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../11/Testing_Hypotheses.html">
   11. Testing Hypotheses
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../11/1/Assessing_Models.html">
     11.1. Assessing Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../11/2/Multiple_Categories.html">
     11.2. Multiple Categories
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../11/3/Decisions_and_Uncertainty.html">
     11.3. Decisions and Uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../11/4/Error_Probabilities.html">
     11.4. Error Probabilities
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../12/Comparing_Two_Samples.html">
   12. Comparing Two Samples
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../12/1/AB_Testing.html">
     12.1. A/B Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../12/2/Deflategate.html">
     12.2. Deflategate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../12/3/Causality.html">
     12.3. Causality
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../13/Estimation.html">
   13. Estimation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../13/1/Percentiles.html">
     13.1. Percentiles
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../13/2/Bootstrap.html">
     13.2. The Bootstrap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../13/3/Confidence_Intervals.html">
     13.3. Confidence Intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../13/4/Using_Confidence_Intervals.html">
     13.4. Using Confidence Intervals
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../14/Why_the_Mean_Matters.html">
   14. Why the Mean Matters
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../14/1/Properties_of_the_Mean.html">
     14.1. Properties of the Mean
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../14/2/Variability.html">
     14.2. Variability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../14/3/SD_and_the_Normal_Curve.html">
     14.3. The SD and the Normal Curve
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../14/4/Central_Limit_Theorem.html">
     14.4. The Central Limit Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../14/5/Variability_of_the_Sample_Mean.html">
     14.5. The Variability of the Sample Mean
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../14/6/Choosing_a_Sample_Size.html">
     14.6. Choosing a Sample Size
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../15/Prediction.html">
   15. Prediction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../15/1/Correlation.html">
     15.1. Correlation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../15/2/Regression_Line.html">
     15.2. The Regression Line
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../15/3/Method_of_Least_Squares.html">
     15.3. The Method of Least Squares
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../15/4/Least_Squares_Regression.html">
     15.4. Least Squares Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../15/5/Visual_Diagnostics.html">
     15.5. Visual Diagnostics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../15/6/Numerical_Diagnostics.html">
     15.6. Numerical Diagnostics
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../16/Inference_for_Regression.html">
   16. Inference for Regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../16/1/Regression_Model.html">
     16.1. A Regression Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../16/2/Inference_for_the_True_Slope.html">
     16.2. Inference for the True Slope
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../16/3/Prediction_Intervals.html">
     16.3. Prediction Intervals
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="../Classification.html">
   17. Classification
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../1/Nearest_Neighbors.html">
     17.1. Nearest Neighbors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2/Training_and_Testing.html">
     17.2. Training and Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3/Rows_of_Tables.html">
     17.3. Rows of Tables
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     17.4. Implementing the Classifier
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5/Accuracy_of_the_Classifier.html">
     17.5. The Accuracy of the Classifier
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6/Multiple_Regression.html">
     17.6. Multiple Regression
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../18/Updating_Predictions.html">
   18. Updating Predictions
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../18/1/More_Likely_than_Not_Binary_Classifier.html">
     18.1. A "More Likely Than Not" Binary Classifier
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../18/2/Making_Decisions.html">
     18.2. Making Decisions
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/chapters/17/4/Implementing_the_Classifier.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/data-8/textbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/data-8/textbook/main?urlpath=tree/chapters/17/4/Implementing_the_Classifier.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        <a class="jupyterhub-button" href="https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/data-8/textbook&urlpath=tree/textbook/chapters/17/4/Implementing_the_Classifier.ipynb&branch=main"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="../../../_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#banknote-authentication">
   17.4.1. Banknote authentication
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiple-attributes">
   17.4.2. Multiple attributes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distance-in-multiple-dimensions">
   17.4.3. Distance in Multiple Dimensions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-plan-for-the-implementation">
   17.4.4. A Plan for the Implementation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation-step-1">
   17.4.5. Implementation Step 1
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation-steps-2-and-3">
   17.4.6. Implementation Steps 2 and 3
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell tag_remove-input docutils container">
</div>
<div class="section" id="implementing-the-classifier">
<h1><span class="section-number">17.4. </span>Implementing the Classifier<a class="headerlink" href="#implementing-the-classifier" title="Permalink to this headline">¶</a></h1>
<p>We are now ready to implement a <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbor classifier based on multiple attributes. We have used only two attributes so far, for ease of visualization. But usually predictions will be based on many attributes. Here is an example that shows how multiple attributes can be better than pairs.</p>
<div class="section" id="banknote-authentication">
<h2><span class="section-number">17.4.1. </span>Banknote authentication<a class="headerlink" href="#banknote-authentication" title="Permalink to this headline">¶</a></h2>
<p>This time we’ll look at predicting whether a banknote (e.g., a $20 bill) is counterfeit or legitimate.  Researchers have put together a data set for us, based on photographs of many individual banknotes: some counterfeit, some legitimate.  They computed a few numbers from each image, using techniques that we won’t worry about for this course.  So, for each banknote, we know a few numbers that were computed from a photograph of it as well as its class (whether it is counterfeit or not).  Let’s load it into a table and take a look.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">banknotes</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">path_data</span> <span class="o">+</span> <span class="s1">&#39;banknote.csv&#39;</span><span class="p">)</span>
<span class="n">banknotes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table border="1" class="dataframe">
    <thead>
        <tr>
            <th>WaveletVar</th> <th>WaveletSkew</th> <th>WaveletCurt</th> <th>Entropy</th> <th>Class</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>3.6216    </td> <td>8.6661     </td> <td>-2.8073    </td> <td>-0.44699</td> <td>0    </td>
        </tr>
        <tr>
            <td>4.5459    </td> <td>8.1674     </td> <td>-2.4586    </td> <td>-1.4621 </td> <td>0    </td>
        </tr>
        <tr>
            <td>3.866     </td> <td>-2.6383    </td> <td>1.9242     </td> <td>0.10645 </td> <td>0    </td>
        </tr>
        <tr>
            <td>3.4566    </td> <td>9.5228     </td> <td>-4.0112    </td> <td>-3.5944 </td> <td>0    </td>
        </tr>
        <tr>
            <td>0.32924   </td> <td>-4.4552    </td> <td>4.5718     </td> <td>-0.9888 </td> <td>0    </td>
        </tr>
        <tr>
            <td>4.3684    </td> <td>9.6718     </td> <td>-3.9606    </td> <td>-3.1625 </td> <td>0    </td>
        </tr>
        <tr>
            <td>3.5912    </td> <td>3.0129     </td> <td>0.72888    </td> <td>0.56421 </td> <td>0    </td>
        </tr>
        <tr>
            <td>2.0922    </td> <td>-6.81      </td> <td>8.4636     </td> <td>-0.60216</td> <td>0    </td>
        </tr>
        <tr>
            <td>3.2032    </td> <td>5.7588     </td> <td>-0.75345   </td> <td>-0.61251</td> <td>0    </td>
        </tr>
        <tr>
            <td>1.5356    </td> <td>9.1772     </td> <td>-2.2718    </td> <td>-0.73535</td> <td>0    </td>
        </tr>
    </tbody>
</table>
<p>... (1362 rows omitted)</p></div></div>
</div>
<p>Let’s look at whether the first two numbers tell us anything about whether the banknote is counterfeit or not.  Here’s a scatterplot:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">color_table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>
    <span class="s1">&#39;Class&#39;</span><span class="p">,</span> <span class="n">make_array</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="s1">&#39;Color&#39;</span><span class="p">,</span> <span class="n">make_array</span><span class="p">(</span><span class="s1">&#39;darkblue&#39;</span><span class="p">,</span> <span class="s1">&#39;gold&#39;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">banknotes</span> <span class="o">=</span> <span class="n">banknotes</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span> <span class="n">color_table</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">banknotes</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">&#39;WaveletVar&#39;</span><span class="p">,</span> <span class="s1">&#39;WaveletCurt&#39;</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s1">&#39;Color&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Implementing_the_Classifier_7_0.png" src="../../../_images/Implementing_the_Classifier_7_0.png" />
</div>
</div>
<p>Pretty interesting!  Those two measurements do seem helpful for predicting whether the banknote is counterfeit or not.  However, in this example you can now see that there is some overlap between the blue cluster and the gold cluster.  This indicates that there will be some images where it’s hard to tell whether the banknote is legitimate based on just these two numbers.  Still, you could use a <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbor classifier to predict the legitimacy of a banknote.</p>
<p>Take a minute and think it through: Suppose we used <span class="math notranslate nohighlight">\(k=11\)</span> (say).  What parts of the plot would the classifier get right, and what parts would it make errors on?  What would the decision boundary look like?</p>
<p>The patterns that show up in the data can get pretty wild.  For instance, here’s what we’d get if used a different pair of measurements from the images:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">banknotes</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">&#39;WaveletSkew&#39;</span><span class="p">,</span> <span class="s1">&#39;Entropy&#39;</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s1">&#39;Color&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Implementing_the_Classifier_9_0.png" src="../../../_images/Implementing_the_Classifier_9_0.png" />
</div>
</div>
<p>There does seem to be a pattern, but it’s a pretty complex one.  Nonetheless, the <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbors classifier can still be used and will effectively “discover” patterns out of this.  This illustrates how powerful machine learning can be: it can effectively take advantage of even patterns that we would not have anticipated, or that we would have thought to “program into” the computer.</p>
</div>
<div class="section" id="multiple-attributes">
<h2><span class="section-number">17.4.2. </span>Multiple attributes<a class="headerlink" href="#multiple-attributes" title="Permalink to this headline">¶</a></h2>
<p>So far I’ve been assuming that we have exactly 2 attributes that we can use to help us make our prediction.  What if we have more than 2?  For instance, what if we have 3 attributes?</p>
<p>Here’s the cool part: you can use the same ideas for this case, too.  All you have to do is make a 3-dimensional scatterplot, instead of a 2-dimensional plot.  You can still use the <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbors classifier, but now computing distances in 3 dimensions instead of just 2.  It just works.  Very cool!</p>
<p>In fact, there’s nothing special about 2 or 3.  If you have 4 attributes, you can use the <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbors classifier in 4 dimensions.  5 attributes?  Work in 5-dimensional space.  And no need to stop there!  This all works for arbitrarily many attributes; you just work in a very high dimensional space.  It gets wicked-impossible to visualize, but that’s OK.  The computer algorithm generalizes very nicely: all you need is the ability to compute the distance, and that’s not hard.  Mind-blowing stuff!</p>
<p>For instance, let’s see what happens if we try to predict whether a banknote is counterfeit or not using 3 of the measurements, instead of just 2.  Here’s what you get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">banknotes</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">&#39;WaveletSkew&#39;</span><span class="p">),</span> 
           <span class="n">banknotes</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">&#39;WaveletVar&#39;</span><span class="p">),</span> 
           <span class="n">banknotes</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">&#39;WaveletCurt&#39;</span><span class="p">),</span> 
           <span class="n">c</span><span class="o">=</span><span class="n">banknotes</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">&#39;Color&#39;</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Implementing_the_Classifier_12_0.png" src="../../../_images/Implementing_the_Classifier_12_0.png" />
</div>
</div>
<p>Awesome!  With just 2 attributes, there was some overlap between the two clusters (which means that the classifier was bound to make some mistakes for pointers in the overlap).  But when we use these 3 attributes, the two clusters have almost no overlap.  In other words, a classifier that uses these 3 attributes will be more accurate than one that only uses the 2 attributes.</p>
<p>This is a general phenomenom in classification.  Each attribute can potentially give you new information, so more attributes sometimes helps you build a better classifier.  Of course, the cost is that now we have to gather more information to measure the value of each attribute, but this cost may be well worth it if it significantly improves the accuracy of our classifier.</p>
<p>To sum up: you now know how to use <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbor classification to predict the answer to a yes/no question, based on the values of some attributes, assuming you have a training set with examples where the correct prediction is known.  The general roadmap is this:</p>
<ol class="simple">
<li><p>identify some attributes that you think might help you predict the answer to the question.</p></li>
<li><p>Gather a training set of examples where you know the values of the attributes as well as the correct prediction.</p></li>
<li><p>To make predictions in the future, measure the value of the attributes and then use <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbor classification to predict the answer to the question.</p></li>
</ol>
</div>
<div class="section" id="distance-in-multiple-dimensions">
<h2><span class="section-number">17.4.3. </span>Distance in Multiple Dimensions<a class="headerlink" href="#distance-in-multiple-dimensions" title="Permalink to this headline">¶</a></h2>
<p>We know how to compute distance in 2-dimensional space. If we have a point at coordinates <span class="math notranslate nohighlight">\((x_0,y_0)\)</span> and another at <span class="math notranslate nohighlight">\((x_1,y_1)\)</span>, the distance between them is</p>
<div class="math notranslate nohighlight">
\[D = \sqrt{(x_0-x_1)^2 + (y_0-y_1)^2}.\]</div>
<p>In 3-dimensional space, the points are <span class="math notranslate nohighlight">\((x_0, y_0, z_0)\)</span> and <span class="math notranslate nohighlight">\((x_1, y_1, z_1)\)</span>, and the formula for the distance between them is</p>
<div class="math notranslate nohighlight">
\[
D = \sqrt{(x_0-x_1)^2 + (y_0-y_1)^2 + (z_0-z_1)^2}
\]</div>
<p>In <span class="math notranslate nohighlight">\(n\)</span>-dimensional space, things are a bit harder to visualize, but I think you can see how the formula generalized: we sum up the squares of the differences between each individual coordinate, and then take the square root of that.</p>
<p>In the last section, we defined the function <code class="docutils literal notranslate"><span class="pre">distance</span></code> which returned the distance between two points. We used it in two-dimensions, but the great news is that the function doesn’t care how many dimensions there are! It just subtracts the two arrays of coordinates (no matter how long the arrays are), squares the differences and adds up, and then takes the square root. To work in multiple dimensions, we don’t have to change the code at all.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="n">point2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the distance between point1 and point2</span>
<span class="sd">    where each argument is an array </span>
<span class="sd">    consisting of the coordinates of the point&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">point1</span> <span class="o">-</span> <span class="n">point2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s use this on a <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Wine">new dataset</a>. The table <code class="docutils literal notranslate"><span class="pre">wine</span></code> contains the chemical composition of 178 different Italian wines. The classes are the grape species, called cultivars. There are three classes but let’s just see whether we can tell Class 1 apart from the other two.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wine</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">path_data</span> <span class="o">+</span> <span class="s1">&#39;wine.csv&#39;</span><span class="p">)</span>

<span class="c1"># For converting Class to binary</span>

<span class="k">def</span> <span class="nf">is_one</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    
<span class="n">wine</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span> <span class="n">wine</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">is_one</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wine</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Class</th> <th>Alcohol</th> <th>Malic Acid</th> <th>Ash</th> <th>Alcalinity of Ash</th> <th>Magnesium</th> <th>Total Phenols</th> <th>Flavanoids</th> <th>Nonflavanoid phenols</th> <th>Proanthocyanins</th> <th>Color Intensity</th> <th>Hue</th> <th>OD280/OD315 of diulted wines</th> <th>Proline</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>1    </td> <td>14.23  </td> <td>1.71      </td> <td>2.43</td> <td>15.6             </td> <td>127      </td> <td>2.8          </td> <td>3.06      </td> <td>0.28                </td> <td>2.29           </td> <td>5.64           </td> <td>1.04</td> <td>3.92                        </td> <td>1065   </td>
        </tr>
        <tr>
            <td>1    </td> <td>13.2   </td> <td>1.78      </td> <td>2.14</td> <td>11.2             </td> <td>100      </td> <td>2.65         </td> <td>2.76      </td> <td>0.26                </td> <td>1.28           </td> <td>4.38           </td> <td>1.05</td> <td>3.4                         </td> <td>1050   </td>
        </tr>
        <tr>
            <td>1    </td> <td>13.16  </td> <td>2.36      </td> <td>2.67</td> <td>18.6             </td> <td>101      </td> <td>2.8          </td> <td>3.24      </td> <td>0.3                 </td> <td>2.81           </td> <td>5.68           </td> <td>1.03</td> <td>3.17                        </td> <td>1185   </td>
        </tr>
        <tr>
            <td>1    </td> <td>14.37  </td> <td>1.95      </td> <td>2.5 </td> <td>16.8             </td> <td>113      </td> <td>3.85         </td> <td>3.49      </td> <td>0.24                </td> <td>2.18           </td> <td>7.8            </td> <td>0.86</td> <td>3.45                        </td> <td>1480   </td>
        </tr>
        <tr>
            <td>1    </td> <td>13.24  </td> <td>2.59      </td> <td>2.87</td> <td>21               </td> <td>118      </td> <td>2.8          </td> <td>2.69      </td> <td>0.39                </td> <td>1.82           </td> <td>4.32           </td> <td>1.04</td> <td>2.93                        </td> <td>735    </td>
        </tr>
        <tr>
            <td>1    </td> <td>14.2   </td> <td>1.76      </td> <td>2.45</td> <td>15.2             </td> <td>112      </td> <td>3.27         </td> <td>3.39      </td> <td>0.34                </td> <td>1.97           </td> <td>6.75           </td> <td>1.05</td> <td>2.85                        </td> <td>1450   </td>
        </tr>
        <tr>
            <td>1    </td> <td>14.39  </td> <td>1.87      </td> <td>2.45</td> <td>14.6             </td> <td>96       </td> <td>2.5          </td> <td>2.52      </td> <td>0.3                 </td> <td>1.98           </td> <td>5.25           </td> <td>1.02</td> <td>3.58                        </td> <td>1290   </td>
        </tr>
        <tr>
            <td>1    </td> <td>14.06  </td> <td>2.15      </td> <td>2.61</td> <td>17.6             </td> <td>121      </td> <td>2.6          </td> <td>2.51      </td> <td>0.31                </td> <td>1.25           </td> <td>5.05           </td> <td>1.06</td> <td>3.58                        </td> <td>1295   </td>
        </tr>
        <tr>
            <td>1    </td> <td>14.83  </td> <td>1.64      </td> <td>2.17</td> <td>14               </td> <td>97       </td> <td>2.8          </td> <td>2.98      </td> <td>0.29                </td> <td>1.98           </td> <td>5.2            </td> <td>1.08</td> <td>2.85                        </td> <td>1045   </td>
        </tr>
        <tr>
            <td>1    </td> <td>13.86  </td> <td>1.35      </td> <td>2.27</td> <td>16               </td> <td>98       </td> <td>2.98         </td> <td>3.15      </td> <td>0.22                </td> <td>1.85           </td> <td>7.22           </td> <td>1.01</td> <td>3.55                        </td> <td>1045   </td>
        </tr>
    </tbody>
</table>
<p>... (168 rows omitted)</p></div></div>
</div>
<p>The first two wines are both in Class 1. To find the distance between them, we first need a table of just the attributes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wine_attributes</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">distance</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_attributes</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_attributes</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>31.265012394048398
</pre></div>
</div>
</div>
</div>
<p>The last wine in the table is of Class 0. Its distance from the first wine is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">distance</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_attributes</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_attributes</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="mi">177</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>506.05936766351834
</pre></div>
</div>
</div>
</div>
<p>That’s quite a bit bigger! Let’s do some visualization to see if Class 1 really looks different from Class 0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wine_with_colors</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span> <span class="n">color_table</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wine_with_colors</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">&#39;Flavanoids&#39;</span><span class="p">,</span> <span class="s1">&#39;Alcohol&#39;</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s1">&#39;Color&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Implementing_the_Classifier_27_0.png" src="../../../_images/Implementing_the_Classifier_27_0.png" />
</div>
</div>
<p>The blue points (Class 1) are almost entirely separate from the gold ones. That is one indication of why the distance between two Class 1 wines would be smaller than the distance between wines of two different classes. We can see a similar phenomenon with a different pair of attributes too:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wine_with_colors</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">&#39;Alcalinity of Ash&#39;</span><span class="p">,</span> <span class="s1">&#39;Ash&#39;</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s1">&#39;Color&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Implementing_the_Classifier_29_0.png" src="../../../_images/Implementing_the_Classifier_29_0.png" />
</div>
</div>
<p>But for some pairs the picture is more murky.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wine_with_colors</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">&#39;Magnesium&#39;</span><span class="p">,</span> <span class="s1">&#39;Total Phenols&#39;</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s1">&#39;Color&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Implementing_the_Classifier_31_0.png" src="../../../_images/Implementing_the_Classifier_31_0.png" />
</div>
</div>
<p>Let’s see if we can implement a classifier based on all of the attributes. After that, we’ll see how accurate it is.</p>
</div>
<div class="section" id="a-plan-for-the-implementation">
<h2><span class="section-number">17.4.4. </span>A Plan for the Implementation<a class="headerlink" href="#a-plan-for-the-implementation" title="Permalink to this headline">¶</a></h2>
<p>It’s time to write some code to implement the classifier.  The input is a <code class="docutils literal notranslate"><span class="pre">point</span></code> that we want to classify.  The classifier works by finding the <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors of <code class="docutils literal notranslate"><span class="pre">point</span></code> from the training set.  So, our approach will go like this:</p>
<ol class="simple">
<li><p>Find the closest <span class="math notranslate nohighlight">\(k\)</span> neighbors of <code class="docutils literal notranslate"><span class="pre">point</span></code>, i.e., the <span class="math notranslate nohighlight">\(k\)</span> wines from the training set that are most similar to <code class="docutils literal notranslate"><span class="pre">point</span></code>.</p></li>
<li><p>Look at the classes of those <span class="math notranslate nohighlight">\(k\)</span> neighbors, and take the majority vote to find the most-common class of wine.  Use that as our predicted class for <code class="docutils literal notranslate"><span class="pre">point</span></code>.</p></li>
</ol>
<p>So that will guide the structure of our Python code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="o">...</span>

<span class="k">def</span> <span class="nf">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">):</span>
    <span class="o">...</span>

<span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">kclosest</span> <span class="o">=</span> <span class="n">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">kclosest</span><span class="o">.</span><span class="n">classes</span> <span class="o">=</span> <span class="n">kclosest</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">majority</span><span class="p">(</span><span class="n">kclosest</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="implementation-step-1">
<h2><span class="section-number">17.4.5. </span>Implementation Step 1<a class="headerlink" href="#implementation-step-1" title="Permalink to this headline">¶</a></h2>
<p>To implement the first step for the kidney disease data, we had to compute the distance from each patient in the training set to <code class="docutils literal notranslate"><span class="pre">point</span></code>, sort them by distance, and take the <span class="math notranslate nohighlight">\(k\)</span> closest patients in the training set.</p>
<p>That’s what we did in the previous section with the point corresponding to Alice. Let’s generalize that code. We’ll redefine <code class="docutils literal notranslate"><span class="pre">distance</span></code> here, just for convenience.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="n">point1</span><span class="p">,</span> <span class="n">point2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the distance between point1 and point2</span>
<span class="sd">    where each argument is an array </span>
<span class="sd">    consisting of the coordinates of the point&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">point1</span> <span class="o">-</span> <span class="n">point2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">all_distances</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns an array of distances</span>
<span class="sd">    between each point in the training set</span>
<span class="sd">    and the new point (which is a row of attributes)&quot;&quot;&quot;</span>
    <span class="n">attributes</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">distance_from_point</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">distance</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">new_point</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">row</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">attributes</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">distance_from_point</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">table_with_distances</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Augments the training table </span>
<span class="sd">    with a column of distances from new_point&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">training</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s1">&#39;Distance&#39;</span><span class="p">,</span> <span class="n">all_distances</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a table of the k rows of the augmented table</span>
<span class="sd">    corresponding to the k smallest distances&quot;&quot;&quot;</span>
    <span class="n">with_dists</span> <span class="o">=</span> <span class="n">table_with_distances</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">)</span>
    <span class="n">sorted_by_distance</span> <span class="o">=</span> <span class="n">with_dists</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s1">&#39;Distance&#39;</span><span class="p">)</span>
    <span class="n">topk</span> <span class="o">=</span> <span class="n">sorted_by_distance</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">topk</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s see how this works on our <code class="docutils literal notranslate"><span class="pre">wine</span></code> data. We’ll just take the first wine and find its five nearest neighbors among all the wines. Remember that since this wine is part of the dataset, it is its own nearest neighbor. So we should expect to see it at the top of the list, followed by four others.</p>
<p>First let’s extract its attributes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">special_wine</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And now let’s find its 5 nearest neighbors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">closest</span><span class="p">(</span><span class="n">wine</span><span class="p">,</span> <span class="n">special_wine</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Class</th> <th>Alcohol</th> <th>Malic Acid</th> <th>Ash</th> <th>Alcalinity of Ash</th> <th>Magnesium</th> <th>Total Phenols</th> <th>Flavanoids</th> <th>Nonflavanoid phenols</th> <th>Proanthocyanins</th> <th>Color Intensity</th> <th>Hue</th> <th>OD280/OD315 of diulted wines</th> <th>Proline</th> <th>Distance</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>1    </td> <td>14.23  </td> <td>1.71      </td> <td>2.43</td> <td>15.6             </td> <td>127      </td> <td>2.8          </td> <td>3.06      </td> <td>0.28                </td> <td>2.29           </td> <td>5.64           </td> <td>1.04</td> <td>3.92                        </td> <td>1065   </td> <td>0       </td>
        </tr>
        <tr>
            <td>1    </td> <td>13.74  </td> <td>1.67      </td> <td>2.25</td> <td>16.4             </td> <td>118      </td> <td>2.6          </td> <td>2.9       </td> <td>0.21                </td> <td>1.62           </td> <td>5.85           </td> <td>0.92</td> <td>3.2                         </td> <td>1060   </td> <td>10.3928 </td>
        </tr>
        <tr>
            <td>1    </td> <td>14.21  </td> <td>4.04      </td> <td>2.44</td> <td>18.9             </td> <td>111      </td> <td>2.85         </td> <td>2.65      </td> <td>0.3                 </td> <td>1.25           </td> <td>5.24           </td> <td>0.87</td> <td>3.33                        </td> <td>1080   </td> <td>22.3407 </td>
        </tr>
        <tr>
            <td>1    </td> <td>14.1   </td> <td>2.02      </td> <td>2.4 </td> <td>18.8             </td> <td>103      </td> <td>2.75         </td> <td>2.92      </td> <td>0.32                </td> <td>2.38           </td> <td>6.2            </td> <td>1.07</td> <td>2.75                        </td> <td>1060   </td> <td>24.7602 </td>
        </tr>
        <tr>
            <td>1    </td> <td>14.38  </td> <td>3.59      </td> <td>2.28</td> <td>16               </td> <td>102      </td> <td>3.25         </td> <td>3.17      </td> <td>0.27                </td> <td>2.19           </td> <td>4.9            </td> <td>1.04</td> <td>3.44                        </td> <td>1065   </td> <td>25.0947 </td>
        </tr>
    </tbody>
</table></div></div>
</div>
<p>Bingo! The first row is the nearest neighbor, which is itself – there’s a 0 in the <code class="docutils literal notranslate"><span class="pre">Distance</span></code> column as expected. All five nearest neighbors are of Class 1, which is consistent with our earlier observation that Class 1 wines appear to be clumped together in some dimensions.</p>
</div>
<div class="section" id="implementation-steps-2-and-3">
<h2><span class="section-number">17.4.6. </span>Implementation Steps 2 and 3<a class="headerlink" href="#implementation-steps-2-and-3" title="Permalink to this headline">¶</a></h2>
<p>Next we need to take a “majority vote” of the nearest neighbors and assign our point the same class as the majority.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">):</span>
    <span class="n">ones</span> <span class="o">=</span> <span class="n">topkclasses</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span> <span class="n">are</span><span class="o">.</span><span class="n">equal_to</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">num_rows</span>
    <span class="n">zeros</span> <span class="o">=</span> <span class="n">topkclasses</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span> <span class="n">are</span><span class="o">.</span><span class="n">equal_to</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">num_rows</span>
    <span class="k">if</span> <span class="n">ones</span> <span class="o">&gt;</span> <span class="n">zeros</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">closestk</span> <span class="o">=</span> <span class="n">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">new_point</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">topkclasses</span> <span class="o">=</span> <span class="n">closestk</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">classify</span><span class="p">(</span><span class="n">wine</span><span class="p">,</span> <span class="n">special_wine</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
<p>If we change <code class="docutils literal notranslate"><span class="pre">special_wine</span></code> to be the last one in the dataset, is our classifier able to tell that it’s in Class 0?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">special_wine</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="mi">177</span><span class="p">)</span>
<span class="n">classify</span><span class="p">(</span><span class="n">wine</span><span class="p">,</span> <span class="n">special_wine</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0
</pre></div>
</div>
</div>
</div>
<p>Yes! The classifier gets this one right too.</p>
<p>But we don’t yet know how it does with all the other wines, and in any case we know that testing on wines that are already part of the training set might be over-optimistic. In the final section of this chapter, we will separate the wines into a training and test set and then measure the accuracy of our classifier on the test set.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters/17/4"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../3/Rows_of_Tables.html" title="previous page"><span class="section-number">17.3. </span>Rows of Tables</a>
    <a class='right-next' id="next-link" href="../5/Accuracy_of_the_Classifier.html" title="next page"><span class="section-number">17.5. </span>The Accuracy of the Classifier</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ani Adhikari and John DeNero<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-148221575-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>